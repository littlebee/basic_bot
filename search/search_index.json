{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"basic_bot","text":"<p>A Python centric, basic robotics platform providing</p> <ul> <li>an ultra lightweight, websockets based pub/sub service</li> <li>hardware support for various motors, servos, sensors</li> <li>support for vision and in frame object detection</li> <li>a simple <code>bb_create</code> script for creating new projects</li> </ul> <p>\ud83d\udcd6 Official API Docs \ud83d\udcda Github Repository \ud83c\udfaf Comments and suggestions welcomed</p>"},{"location":"#status","title":"Status","text":"<p> This is still a work in progress </p>"},{"location":"#updates","title":"Updates","text":"<p>20250128 - We have online hosted docs! \ud83c\udf89</p> <p>20250127 - It can see! Working basic_bot.services.vision_cv service that uses open cv2 and tensorflow lite.  Still trying to decide what else is needed in the beta version - support for GPIO in?  Doing this in parallel with implementing the first working example of using basic_bot - daphbot-due</p> <p>20250120 - Basic working central_hub and web_server services merged.  Working on adding basic motor controller service similar to Scatbot.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>On your development computer (not your bot's onboard computer; see later in this doc)</p> <p><pre><code># first create or activate a python venv\npython -m venv my_new_project_venv\nsource my_new_project_venv/bin/activate\n\n# installs the basic_bot package\npython -m pip install git+https://github.com/littlebee/basic_bot.git@main\n\n# creates a new robotics project\nbb_create my_new_robot_project\n</code></pre> The bb_create command above will</p> <ul> <li>add files for the basic shell of a webapp</li> <li>add test of webapp</li> <li>add files for example service that increments a counter</li> <li>add test for example service</li> <li>add start.sh, stop.sh, upload.sh scripts to my_new_robot_project/</li> <li>add build.sh and test.sh scripts to my_new_robot_project/</li> <li>run build.sh and test.sh scripts</li> </ul> <p>After <code>bb_create</code> finishes you should be able to</p> <pre><code>cd my_new_robot_project\n</code></pre>"},{"location":"#everything-created-is-yours-to-edit","title":"Everything created is yours to edit","text":"<p><code>bb_create</code> is single use per project and will not overwrite an existing directory.</p> <p>The created app should run on any OS supported by Python &gt;= 3.9 with a Posix compliant shell.  I've tested this assertion on macOS &gt;= 14.7, on Raspberry PI 4 with Raspian Bullseye, and on the Raspberry Pi 5 w/ Debian Bookworm.  Additionally, the CI/CD test system for this project validates that everything works on <code>ubuntu-latest</code>.</p> <p>Python &gt;= 3.9 is required.  It's also a good idea to make sure <code>python</code> and <code>pip</code> commands are pointing to the correct version (3.9) if you are using a virtual environment like Anaconda or Miniconda.</p> <p>To build and use the created <code>./webapp</code> example, you must have Node.js &gt;= v20.18, and <code>npm</code> needs to be in your path.</p> <p>From there, check out the .sh files in the root of my_new_robot_project.  Commands to build, start in the background, run integration tests for the Python example \"worthless_counter\" service, and run example integration test.</p>"},{"location":"#kicking-the-tires-locally","title":"Kicking the tires locally","text":"<p>You can start the services locally in the background (from your project root dir on dev machine): <pre><code>./start.sh\n</code></pre> ... which will start all services in the `` file.  Each service runs in as a process.</p> <p>You can start a hot development web server that will show your changes to the webapp/ code in near real time most of the time :). <pre><code>cd webapp\nnpm run dev\n</code></pre></p> <p>For more information about hacking on the code, running tests, and debugging, see CONTRIBUTING.md.</p>"},{"location":"#upload-to-your-robot","title":"Upload to your robot","text":"<p>Also included is an example <code>upload.sh</code> script that can be used to upload your code to your robot's onboard computer.  The example <code>upload.sh</code> uses <code>rsync</code> and requires that both your local and onboard computer need to have SSH installed and properly set up.  If you can <code>ssh my_rad_robot.local</code> or by IP, you should be able to use rsync and the upload example.</p> <p>Example: <pre><code>./upload.sh pi@my_raspberry_bot.local /home/pi/my_bot\n</code></pre></p> <ul> <li><code>pi</code> above is the username</li> <li><code>my_raspberry_bot.local</code> above is the hostname to upload can be replaced by IPAddress</li> <li><code>/home/pi/my_bot</code> is the optional directory you wish to upload to. If not specified, the default is <code>home/$USER/basic_bot</code>. The directory will be created if it does not already exist.</li> </ul> <p>Notes:</p> <p>I work on a macbook and use iTerm2 for terminal.  One of the additions I like to make to the standard setup, is to use the same user name on the Pi that I use locally on my mac.  This allows not having to type <code>ssh raspberry@mybot.local</code>, because the name is the same just <code>ssh mybot.local</code> works.</p> <p>I also like to add my public key to the <code>~/.ssh/authorized_keys</code> file on the remote SBC. This will stop it from prompting you for the password on every SSH command (upload.sh uses rsync which uses ssh).   I made a gist of the script I use to upload my public key to new boards.</p>"},{"location":"#run-the-software-on-your-robot","title":"Run the software on your robot","text":"<p>First <code>ssh</code> into the onboard computer and <code>cd path/uploaded/to</code>.</p>"},{"location":"#create-and-source-python-venv","title":"Create and source python venv","text":"<p>Unfortunately some things like Picamera2 require the shipped python libs so be sure to include <code>--system-site-packages</code> below when running on Raspberry Pi 4 or 5.</p> <pre><code>python -m venv --system-site-packages my_new_project_venv\nsource my_new_project_venv/bin/activate\n</code></pre>"},{"location":"#upgrade-pip","title":"Upgrade pip","text":"<p>basic_bot package install requires a newer version of pip than ships with Python 3.9 on Bullseye and possibly other Linux based OS.  Generally, it's a good idea to get the latest <code>pip</code> <pre><code>python -m pip install --upgrade pip\n</code></pre></p>"},{"location":"#install-basic_bot-onboard","title":"Install basic_bot onboard","text":"<pre><code>python -m pip install git+https://github.com/littlebee/basic_bot.git@main\n</code></pre>"},{"location":"#add-bb_env-export","title":"Add BB_ENV export","text":"<p>The onboard computer of the robot is considered the \"production\" environment. Some basic_bot modules (like motor controllers) may load mock versions of themselves when not running in the production environment. This is intended to keep from accidentally, for example, running the bot off of your workbench or your robotic arm smacking you in the face because you ran tests or diagnostics on the onboard system.</p> <p>The following command will add <code>BB_ENV=production</code> to the environment variables for your OS user when running a bash shell: <pre><code>echo \"export BB_ENV=production\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre></p> <p>You can also prefix env vars on the command line in most shells: <pre><code>BB_ENV=production bb_start\n</code></pre></p> <p>If you plan on starting your robot software at boot using <code>/etc/rc.local</code>, just add the export below to rc.local before starting your robot. <pre><code>export BB_ENV=production\n\n/path/to/scriptThatStartsMyBot.sh\n</code></pre></p>"},{"location":"#start-the-onboard-services","title":"Start the onboard services","text":"<p>Then <code>cd</code> to the directory you uploaded to: <pre><code>bb_start\n</code></pre> will start all of the services in  <code>./basic_bot.yml</code> as individual processes running detached.  If your shell/terminal is closed, they will keep running.</p>"},{"location":"#debugging-issues","title":"Debugging issues","text":"<p>You can debug issues with a service by looking at the ./logs/* files for each service.</p> <p>basic_bot also provides several scripts in the <code>basic_bot.debug</code> package that can be used to diagnose 3rd party software like opencv and Tensor Flow Lite.</p> <p>First stop all services: <pre><code>bb_stop\n</code></pre></p> <p>To see if your opencv installation is working correctly with your robot's camera: <pre><code>python -m python -m basic_bot.debug.test_opencv_capture\n</code></pre></p> <p>The above may fail because opencv capture doesn't work with libcamera (Pi4 and Pi5) yet.  Try this on Raspbery: <pre><code>python -m python -m basic_bot.debug.test_picam2_opencv_capture\n</code></pre></p>"},{"location":"#adding-your-own-ui-too","title":"Adding your own UI, too?","text":"<p>Ya sure, you betcha!  The created <code>my_new_robot_project/webapp</code> is just an example of how to interact with central_hub.  You can add your own content or completely replace it with say Next.js if that is your thing.</p> <p>The created ./webapp is currently a Vite app, created with <code>yarn create vite</code>.   The <code>-m basic_bot.services.web_server</code> service currently is hard coded (TODO: fix) to look in ./webapp/dist for index.html and the rest is up to the app.</p> <p>If you're want to use another frontend framework, you should be able to symbolic link where ever that framework's bundler puts it's build assets like index.html to <code>./webapp/dist</code></p>"},{"location":"#build-non-python-services","title":"Build non Python services?","text":"<p>Sure, anything that you can run from a shell prompt, you can use as a service and <code>bb_start</code> and <code>bb_stop</code> will manage it in the background.   All you need is websockets and JSON parsing support in your language of choice and you can do it!</p> <p>See central_hub service docs for more information on the interface for subscribing and publishing state.</p>"},{"location":"#examples","title":"Examples","text":"<p>daphbot-due (In progress) A robot for keeping your pet off of the kitchen counter.</p> <p>stongarm (TBD: need to back port it to using bb). A robotic arm application with visual simulation.</p> <p>svgarm (TBD: dream stage) A robotic arm that can be mounted above a whiteboard and draw svgs sent to it.</p>"},{"location":"#will-it-run-on-windows","title":"Will it run on Windows?","text":"<p>Answer is IDK, yet.  Wherever possible, I've tried to use the cross-platform, pure Python, file system and os abstractions.</p> <p>See also, https://github.com/littlebee/basic_bot/issues/61</p>"},{"location":"#how-it-all-works","title":"How it all works","text":"<p>Start with the docs for [central_hub service docs]</p>"},{"location":"#todo-add-updated-version-of-how-it-all-works-from-scatbot","title":"TODO add updated version of How It All Works from scatbot","text":"<p>!()[https://github.com/littlebee/scatbot/blob/c2800de8906b14173201d16030e8d390157eb641/docs/img/scatbot-systems-diagram.png]</p>"},{"location":"Api%20Docs/commons/base_camera/","title":"base_camera","text":""},{"location":"Api%20Docs/commons/base_camera/#basic_botcommonsbase_camera","title":"basic_bot.commons.base_camera","text":"<p>This was originally pilfered from https://github.com/adeept/Adeept_RaspTank/blob/a6c45e8cc7df620ad8977845eda2b839647d5a83/server/base_camera.py</p> <p>Which looks like it was in turn pilfered from https://blog.miguelgrinberg.com/post/flask-video-streaming-revisited</p> <p>Thank you, @adeept and @miguelgrinberg!</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#cameraevent-objects","title":"CameraEvent Objects","text":"<pre><code>class CameraEvent(object)\n</code></pre> <p>An Event-like class that signals all active clients when a new frame is available.</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#wait","title":"wait","text":"<pre><code>def wait() -&gt; bool\n</code></pre> <p>Invoked from each client's thread to wait for the next frame.</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#set","title":"set","text":"<pre><code>def set() -&gt; None\n</code></pre> <p>Invoked by the camera thread when a new frame is available.</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#clear","title":"clear","text":"<pre><code>def clear() -&gt; None\n</code></pre> <p>Invoked from each client's thread after a frame was processed.</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#basecamera-objects","title":"BaseCamera Objects","text":"<pre><code>class BaseCamera(object)\n</code></pre> <p>BaseCamera is an abstract base class that for camera implementations.  It creates a background thread that reads frames from the camera and signals when a new frame is available.</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#frame","title":"frame","text":"<p>current frame is stored here by background thread</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#__init__","title":"__init__","text":"<pre><code>def __init__() -&gt; None\n</code></pre> <p>Start the background camera thread if it isn't running yet.</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#get_frame","title":"get_frame","text":"<pre><code>def get_frame() -&gt; Optional[bytes]\n</code></pre> <p>Return the current camera frame.</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#frames","title":"frames","text":"<pre><code>@staticmethod\ndef frames() -&gt; Generator[bytes, None, None]\n</code></pre> <p>\"Generator that returns frames from the camera.</p> <p></p>"},{"location":"Api%20Docs/commons/base_camera/#stats","title":"stats","text":"<pre><code>@classmethod\ndef stats(cls) -&gt; dict[str, float]\n</code></pre> <p>Return the fps stats dictionary.</p>"},{"location":"Api%20Docs/commons/camera_opencv/","title":"camera_opencv","text":""},{"location":"Api%20Docs/commons/camera_opencv/#basic_botcommonscamera_opencv","title":"basic_bot.commons.camera_opencv","text":""},{"location":"Api%20Docs/commons/camera_opencv/#camera-objects","title":"Camera Objects","text":"<pre><code>class Camera(BaseCamera)\n</code></pre> <p>This class implements the BaseCamera interface using OpenCV.</p> <p>Usage:</p> <pre><code>from basic_bot.commons.camera_opencv import Camera\n\ncamera = Camera()\n# get_frame() is from BaseCamera and returns a single frame\nframe = camera.get_frame()\n# you can then used the image frame for example:\njpeg = cv2.imencode(\".jpg\", frame)[1].tobytes()\n</code></pre> <p></p>"},{"location":"Api%20Docs/commons/camera_opencv/#frames","title":"frames","text":"<pre><code>@staticmethod\ndef frames() -&gt; Generator[bytes, None, None]\n</code></pre> <p>Generator function that yields frames from the camera. Required by BaseCamera</p>"},{"location":"Api%20Docs/commons/camera_picamera/","title":"camera_picamera","text":""},{"location":"Api%20Docs/commons/camera_picamera/#basic_botcommonscamera_picamera","title":"basic_bot.commons.camera_picamera","text":""},{"location":"Api%20Docs/commons/camera_picamera/#camera-objects","title":"Camera Objects","text":"<pre><code>class Camera(BaseCamera)\n</code></pre> <p>This class implements the BaseCamera interface using Picamera2.</p> <p>To use the Picamera2, you need to be running on a Raspberry Pi Bullseye or Bookworm with a ribbon cable camera module installed.</p> <p>To use with the vision service, you need to set the environment variable <code>BB_CAMERA_MODULE=basic_bot.commons.camera_picamera</code> before the vision service is started.  You can al</p> <pre><code>- name: \"vision\"\n  run: \"python -m basic_bot.services.vision\"\n  production_env:\n    BB_CAMERA_MODULE: \"basic_bot.commons.camera_picamera\"\n</code></pre> <p>By default the vision service uses opencv camera capture which ATM works on Bullseye and Bookworm with USB camera, but not for ribbon cable cameras. See also the Installation Guide for Bookworm.</p> <p>Direct Usage:</p> <pre><code>from basic_bot.commons.camera_picamera import Camera\n\ncamera = Camera()\n# get_frame() is from BaseCamera and returns a single frame\nframe = camera.get_frame()\n# you can then used the image frame for example:\njpeg = cv2.imencode(\".jpg\", frame)[1].tobytes()\n</code></pre> <p>See vision service for another example usage.</p> <p></p>"},{"location":"Api%20Docs/commons/camera_picamera/#frames","title":"frames","text":"<pre><code>@staticmethod\ndef frames() -&gt; Generator[bytes, None, None]\n</code></pre> <p>Generator function that yields frames from the camera. Required by BaseCamera</p>"},{"location":"Api%20Docs/commons/coco_lables/","title":"coco_lables","text":""},{"location":"Api%20Docs/commons/coco_lables/#basic_botcommonscoco_lables","title":"basic_bot.commons.coco_lables","text":"<p>These are the text labels used for most(?) TF object detection models.</p>"},{"location":"Api%20Docs/commons/config_file_schema/","title":"config_file_schema","text":""},{"location":"Api%20Docs/commons/config_file_schema/#basic_botcommonsconfig_file_schema","title":"basic_bot.commons.config_file_schema","text":"<p>This module contains the schema for the configuration file that bb_start reads.</p> <p>It can be used with jsonvalidate to validate the configuration file.</p>"},{"location":"Api%20Docs/commons/constants/","title":"constants","text":""},{"location":"Api%20Docs/commons/constants/#basic_botcommonsconstants","title":"basic_bot.commons.constants","text":"<p>basic_bot.commons.constants can be imported to access the constants used by basic_bot services. The constants are set by default and all can be overridden by setting environment variables.</p> <p>If you are using bb_start to start the services, you can set the BB_ environment variables in the basic_bot.yml file. See the bb_start documentation</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_env","title":"BB_ENV","text":"<p>Default: \"development\"</p> <p>Test runner should set this to \"test\".</p> <p>Set this to \"production\" on the onboard computer to run the bot in production mode.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_log_all_messages","title":"BB_LOG_ALL_MESSAGES","text":"<p>Default: False</p> <p>Set this to True to log all messages sent and received by services to each of their respective log files.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_hub_port","title":"BB_HUB_PORT","text":"<p>Default: 5100</p> <p>This is the websocket port that the central hub listens on.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_hub_uri","title":"BB_HUB_URI","text":"<p>Default: <code>f\"ws://127.0.0.1:{BB_HUB_PORT}/ws\"</code></p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_motor_i2c_address","title":"BB_MOTOR_I2C_ADDRESS","text":"<p>default: 0x60</p> <p>Used by basic_bot.services.motor_control_2w to connect to the i2c motor controller.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_left_motor_channel","title":"BB_LEFT_MOTOR_CHANNEL","text":"<p>default: 1</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_right_motor_channel","title":"BB_RIGHT_MOTOR_CHANNEL","text":"<p>default: 2</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_servo_config_file","title":"BB_SERVO_CONFIG_FILE","text":"<p>See api docs for servo control services</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_system_stats_sample_interval","title":"BB_SYSTEM_STATS_SAMPLE_INTERVAL","text":"<p>In seconds, the interval at which the system stats service samples the system and publishes system stats to the central hub.</p> <p>default: 1</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_camera_module","title":"BB_CAMERA_MODULE","text":"<p>default: \"basic_bot.commons.camera_opencv\" or     \"basic_bot.test_helpers.camera_mock\" when BB_ENV=test</p> <p>When using a ribbon cable camera on a Raspberry Pi 4/5, set this to \"basic_bot.commons.camera_picamera\".</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_camera_channel","title":"BB_CAMERA_CHANNEL","text":"<p>default: 0</p> <p>This is the camera channel to use. 0 is the default camera.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_camera_rotation","title":"BB_CAMERA_ROTATION","text":"<p>default: 0</p> <p>This is the camera rotation in degrees. 0 is the default rotation.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_camera_fps","title":"BB_CAMERA_FPS","text":"<p>default: 30</p> <p>This is the camera setting frames per second.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_vision_width","title":"BB_VISION_WIDTH","text":"<p>default: 640</p> <p>In pixels, this is the width of the camera frame to capture.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_vision_height","title":"BB_VISION_HEIGHT","text":"<p>default: 480</p> <p>In pixels, this is the height of the camera frame to capture.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_vision_fov","title":"BB_VISION_FOV","text":"<p>default: 62</p> <p>This is the field of view of the camera in degrees. Depends on camera; RPi v2 cam is 62deg.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_object_detection_threshold","title":"BB_OBJECT_DETECTION_THRESHOLD","text":"<p>default: 0.5</p> <p>This is the object detection threshold percentage 0 - 1; higher = greater confidence.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_enable_coral_tpu","title":"BB_ENABLE_CORAL_TPU","text":"<p>default: False</p> <p>Set this to True to enable the Coral USB TPU for object detection.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_tflite_model","title":"BB_TFLITE_MODEL","text":"<p>default: \"./models/tflite/ssd_mobilenet_v1_coco_quant_postprocess.tflite\"</p> <p>Which model to use for object detection when BB_ENABLE_CORAL_TPU is false. Default is the model from the coral site which is faster than the model from the tensorflow hub</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_tflite_model_coral","title":"BB_TFLITE_MODEL_CORAL","text":"<p>default: \"./models/tflite/ssd_mobilenet_v1_coco_quant_postprocess_edgetpu.tflite\"</p> <p>Which model to use for object detection when BB_ENABLE_CORAL_TPU is true.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_tflite_threads","title":"BB_TFLITE_THREADS","text":"<p>default: 3</p> <p>Number of threads to use for tflite detection.  Testing object detection on a Rasberry PI 5, without any other CPU or memory pressure, 4 tflite threads was only 1 fps faster (29.5fps) than 3 threads (28.6fps).  2 threads was 22fps.</p> <p>Warning: Setting this too high can actually reduce the object detection  frame rate. In the case of daphbot_due, which has a pygame based onboard UI service that has a configurable render frame rate, the tflite detection running on 4 threads was reduced to about 12 fps when the render frame rate was set to 30fps.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_vision_port","title":"BB_VISION_PORT","text":"<p>default: 5802 when BB_ENV=test; 5801 otherwise</p> <p>This is the HTTP port that the vision service listens  on for video streaming and REST api.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_disable_recognition_provider","title":"BB_DISABLE_RECOGNITION_PROVIDER","text":"<p>default: False</p> <p>Set this to True to disable the recognition provider.</p> <p></p>"},{"location":"Api%20Docs/commons/constants/#bb_video_path","title":"BB_VIDEO_PATH","text":"<p>default: \"./recorded_video\"</p> <p>The path where the vision service saves recorded video.</p>"},{"location":"Api%20Docs/commons/env/","title":"env","text":""},{"location":"Api%20Docs/commons/env/#basic_botcommonsenv","title":"basic_bot.commons.env","text":"<p>Simple utility functions to get typed environment variables with default values.</p> <p>Usage:</p> <pre><code>from basic_bot.commons import env\n\nMY_INT = env.env_int(\"MY_INT\", 5800)\nMY_STRING = env.env_string(\"MY_STRING\", \"default\")\nMY_FLOAT = env.env_float(\"MY_FLOAT\", 3.14)\nMY_BOOL = env.env_bool(\"MY_BOOL\", True)\n</code></pre> <p></p>"},{"location":"Api%20Docs/commons/env/#env_string","title":"env_string","text":"<pre><code>def env_string(name: str, default: str) -&gt; str\n</code></pre> <p>Parse environment variable as string with default value.</p> <p></p>"},{"location":"Api%20Docs/commons/env/#env_int","title":"env_int","text":"<pre><code>def env_int(name: str, default: int) -&gt; int\n</code></pre> <p>Parse environment variable as int with default value.</p> <p></p>"},{"location":"Api%20Docs/commons/env/#env_float","title":"env_float","text":"<pre><code>def env_float(name: str, default: float) -&gt; float\n</code></pre> <p>Parse environment variable as float with default value.</p> <p></p>"},{"location":"Api%20Docs/commons/env/#env_bool","title":"env_bool","text":"<pre><code>def env_bool(name: str, default: bool) -&gt; bool\n</code></pre> <p>Parse environment variable as bool with default value.</p>"},{"location":"Api%20Docs/commons/fps_stats/","title":"fps_stats","text":""},{"location":"Api%20Docs/commons/fps_stats/#basic_botcommonsfps_stats","title":"basic_bot.commons.fps_stats","text":""},{"location":"Api%20Docs/commons/fps_stats/#fpsstats-objects","title":"FpsStats Objects","text":"<pre><code>class FpsStats()\n</code></pre> <p>FpsStats - A class to track overall and floating frames per seconds.  Floating fps is calculated over the last 60 seconds.</p> <p>Instantiate the class to start tracking the stats. Call increment() for each frame read. Call stats() to get the stats.</p> <p></p>"},{"location":"Api%20Docs/commons/fps_stats/#increment","title":"increment","text":"<pre><code>def increment() -&gt; None\n</code></pre> <p>Increment the frame count and calculate the floating fps</p> <p></p>"},{"location":"Api%20Docs/commons/fps_stats/#stats","title":"stats","text":"<pre><code>def stats() -&gt; dict[str, float]\n</code></pre> <p>Return the stats as a dictionary</p>"},{"location":"Api%20Docs/commons/hub_state/","title":"hub_state","text":""},{"location":"Api%20Docs/commons/hub_state/#basic_botcommonshub_state","title":"basic_bot.commons.hub_state","text":""},{"location":"Api%20Docs/commons/hub_state/#hubstate-objects","title":"HubState Objects","text":"<pre><code>class HubState()\n</code></pre> <p>This class manages the local state of the hub.  It is initialized with a default initial state and can be updated with new state data.</p> <p></p>"},{"location":"Api%20Docs/commons/hub_state/#__init__","title":"__init__","text":"<pre><code>def __init__(default_state: Dict[str, Any] = {}) -&gt; None\n</code></pre> <p>Initializes the hub state with the default state.</p> <p></p>"},{"location":"Api%20Docs/commons/hub_state/#get","title":"get","text":"<pre><code>def get(keys_requested: List[str]) -&gt; Dict[str, Any]\n</code></pre> <p>Return the requested state data for a list of state keys.</p> <p></p>"},{"location":"Api%20Docs/commons/hub_state/#update_state_from_message_data","title":"update_state_from_message_data","text":"<pre><code>def update_state_from_message_data(message_data: Dict[str, Any]) -&gt; None\n</code></pre> <p>Update state from received message data.</p> <p></p>"},{"location":"Api%20Docs/commons/hub_state/#serialize_state","title":"serialize_state","text":"<pre><code>def serialize_state(keys_requested: Optional[List[str]] = None) -&gt; str\n</code></pre> <p>Serialize the current state to JSON.</p>"},{"location":"Api%20Docs/commons/hub_state_monitor/","title":"hub_state_monitor","text":""},{"location":"Api%20Docs/commons/hub_state_monitor/#basic_botcommonshub_state_monitor","title":"basic_bot.commons.hub_state_monitor","text":""},{"location":"Api%20Docs/commons/hub_state_monitor/#hubstatemonitor-objects","title":"HubStateMonitor Objects","text":"<pre><code>class HubStateMonitor()\n</code></pre> <p>This class updates the process local copy of the hub state as subscribed keys are changed.  It starts a thread to listen for state updates from the central hub and applies them to the local state via hub_state.update_state_from_message_data.</p> <p>Before applying the state update, it calls the on_state_update callback if it is provided.  This allows the caller to do something with the state update before it is applied to the local state and to see the difference in current state vs. to be applied state.</p> <p>The state update is applied to the local state via hub_state.update_state_from_message_data regardless of whether the on_state_update callback is provided or the value it returns.  To alter the state you should alway send an <code>updateState</code> message to the central hub.</p> <p>Usage: <pre><code>from basic_bot.commons.hub_state import HubState\nfrom basic_bot.commons.hub_state_monitor import HubStateMonitor\n\nhub_state = HubState({\"test_key\": \"test_value\"})\nmonitor = HubStateMonitor(hub_state, \"test_identity\", [\"test_key\"])\nmonitor.start()\n</code></pre> The above example will start a background thread that listens for state updates to the \"test_key\" key from the central hub and updates the local state with the new value.</p> <p>For a more complex example using callbacks, see usage in daphbot example - daphbot_service</p> <p></p>"},{"location":"Api%20Docs/commons/hub_state_monitor/#__init__","title":"__init__","text":"<pre><code>def __init__(\n    hub_state: HubState,\n    identity: str,\n    subscribed_keys: Union[List[str], Literal[\"*\"]],\n    on_connect: Optional[Callable[\n        [WebSocketClientProtocol],\n        None,\n    ]] = None,\n    on_state_update: Optional[Callable[\n        [\n            WebSocketClientProtocol,\n            str,\n            dict,\n        ],\n        None,\n    ]] = None\n) -&gt; None\n</code></pre> <p>Instantiate a HubStateMonitor object.</p> <p>Note that subscribed_keys may be an empty list if you just want to publish state updates to the central hub and not receive any state updates.</p>"},{"location":"Api%20Docs/commons/log/","title":"log","text":""},{"location":"Api%20Docs/commons/log/#basic_botcommonslog","title":"basic_bot.commons.log","text":"<p>Simple methods for logging messages to console with timestamps and \"INFO\", \"DEBUG\", \"ERROR\" prefixes.</p> <p>These methods ensure that the last message is flushed to console.</p> <p><code>bb_start</code>, which is used to run services in the background, will redirect all stdout and stderr to a log file.</p> <p></p>"},{"location":"Api%20Docs/commons/log/#debug","title":"debug","text":"<pre><code>def debug(message: str) -&gt; None\n</code></pre> <p>Flush DEBUG: message to console only in development and test environments</p> <p></p>"},{"location":"Api%20Docs/commons/log/#info","title":"info","text":"<pre><code>def info(message: str) -&gt; None\n</code></pre> <p>Flush INFO: message to console</p> <p></p>"},{"location":"Api%20Docs/commons/log/#error","title":"error","text":"<pre><code>def error(message: str) -&gt; None\n</code></pre> <p>Flush ERROR: message to console</p>"},{"location":"Api%20Docs/commons/messages/","title":"messages","text":""},{"location":"Api%20Docs/commons/messages/#basic_botcommonsmessages","title":"basic_bot.commons.messages","text":"<p>Functions from this module are used to send messages to the central_hub via websockets.</p> <p></p>"},{"location":"Api%20Docs/commons/messages/#messagetype-objects","title":"MessageType Objects","text":"<pre><code>class MessageType(Enum)\n</code></pre> <p>Message types for communication to/from central_hub.</p> <p></p>"},{"location":"Api%20Docs/commons/messages/#send_message","title":"send_message","text":"<pre><code>async def send_message(websocket: Any, message: Dict[str, Any]) -&gt; None\n</code></pre> <p>Send a message in the form of a dictionary to central_hub.</p> <p></p>"},{"location":"Api%20Docs/commons/messages/#send_identity","title":"send_identity","text":"<pre><code>async def send_identity(websocket: Any, name: str) -&gt; None\n</code></pre> <p>Send the <code>identity</code> type message to central_hub.  Name should uniquely identify the service.</p> <p></p>"},{"location":"Api%20Docs/commons/messages/#send_subscribe","title":"send_subscribe","text":"<pre><code>async def send_subscribe(\n        websocket: Any, subscriptionNames: Union[List[str],\n                                                 Literal[\"*\"]]) -&gt; None\n</code></pre> <p>Send the <code>subscribeState</code> message type to central_hub.</p> <p><code>subscriptionNames</code> should be an array of keys to subscribe or \"*\" to subscribe to all keys.</p> <p></p>"},{"location":"Api%20Docs/commons/messages/#send_get_state","title":"send_get_state","text":"<pre><code>async def send_get_state(websocket: Any,\n                         keys: Optional[List[str]] = []) -&gt; None\n</code></pre> <p>Send the <code>getState</code> message type to central_hub optionally specifying a list of keys to get the state.</p> <p></p>"},{"location":"Api%20Docs/commons/messages/#send_update_state","title":"send_update_state","text":"<pre><code>async def send_update_state(websocket: Any, stateData: Dict[str, Any]) -&gt; None\n</code></pre> <p>Send the <code>updateState</code> message type to central_hub with the key-&gt;value state data to update.</p>"},{"location":"Api%20Docs/commons/persist_state/","title":"persist_state","text":""},{"location":"Api%20Docs/commons/persist_state/#basic_botcommonspersist_state","title":"basic_bot.commons.persist_state","text":""},{"location":"Api%20Docs/commons/persist_state/#persiststate-objects","title":"PersistState Objects","text":"<pre><code>class PersistState()\n</code></pre> <p>Class to handle persisting the bot state to a file.</p> <p></p>"},{"location":"Api%20Docs/commons/persist_state/#__init__","title":"__init__","text":"<pre><code>def __init__(hub_state: HubState, file_path: str,\n             persisted_state_keys: List[str]) -&gt; None\n</code></pre> <p>Initialize the PersistState object and load the persisted state from the file if it exists.</p> <p>Arguments:</p> <ul> <li>hub_state: the hub state object instance to persist</li> <li>file_path: the file path to persist the state to</li> <li>persisted_state_keys: the keys to persist to the file</li> </ul> <p>Usage:</p> <pre><code>from basic_bot.commons.hub_state import HubState\nfrom basic_bot.commons.persist_state import PersistState\n\n# initialize the hub state\nhub_state = HubState({\n    \"test_key\": \"test_value\",\n    \"some_other_key\": \"some_other_value\"\n})\n\n# hub_state will be updated with the persisted state if the file exists\npersist = PersistState(hub_state, \"./my_service_persisted.json\", [\"test_key\"])\n\n# ... do something that updates the hub_state\n\npersist.persist_state()\n</code></pre> <p></p>"},{"location":"Api%20Docs/commons/persist_state/#persist_state","title":"persist_state","text":"<pre><code>def persist_state() -&gt; None\n</code></pre> <p>Persist the current state to a file.</p>"},{"location":"Api%20Docs/commons/recognition_provider/","title":"recognition_provider","text":""},{"location":"Api%20Docs/commons/recognition_provider/#basic_botcommonsrecognition_provider","title":"basic_bot.commons.recognition_provider","text":""},{"location":"Api%20Docs/commons/recognition_provider/#recognitionprovider-objects","title":"RecognitionProvider Objects","text":"<pre><code>class RecognitionProvider()\n</code></pre> <p>This singleton class detects objects in frames it gets from the camera object passed to the constructor.</p> <p>It uses the TFLiteDetect class to detect objects in the frames.</p> <p>It sends the detected objects to the central hub via a websocket connection using the <code>recognition</code> key</p> <p>To use, simply instantiate it:</p> <pre><code>from basic_bot.commons.recognition_provider import RecognitionProvider\nfrom basic_bot.commons.camera_opencv import OpenCvCamera\n\ncamera = OpenCvCamera()\nrecognition_provider = RecognitionProvider(camera)\n</code></pre> <p></p>"},{"location":"Api%20Docs/commons/recognition_provider/#__init__","title":"__init__","text":"<pre><code>def __init__(camera: Any) -&gt; None\n</code></pre> <p>Constructor</p> <p></p>"},{"location":"Api%20Docs/commons/recognition_provider/#get_objects","title":"get_objects","text":"<pre><code>def get_objects() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Return the last objects seen</p> <p></p>"},{"location":"Api%20Docs/commons/recognition_provider/#get_next_objects","title":"get_next_objects","text":"<pre><code>def get_next_objects() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Wait for and return the next objects seen</p> <p></p>"},{"location":"Api%20Docs/commons/recognition_provider/#pause","title":"pause","text":"<pre><code>def pause() -&gt; None\n</code></pre> <p>Pause the recognition thread</p> <p></p>"},{"location":"Api%20Docs/commons/recognition_provider/#resume","title":"resume","text":"<pre><code>def resume() -&gt; None\n</code></pre> <p>Resume the recognition thread</p> <p></p>"},{"location":"Api%20Docs/commons/recognition_provider/#stats","title":"stats","text":"<pre><code>@classmethod\ndef stats(cls) -&gt; Dict[str, Any]\n</code></pre> <p>Return the fps stats dictionary.</p>"},{"location":"Api%20Docs/commons/servo_config/","title":"servo_config","text":""},{"location":"Api%20Docs/commons/servo_config/#basic_botcommonsservo_config","title":"basic_bot.commons.servo_config","text":"<p>Utility functions for reading and validating servo configuration from the <code>servo_config.yml</code> file.</p> <p></p>"},{"location":"Api%20Docs/commons/servo_config/#read_servo_config","title":"read_servo_config","text":"<pre><code>def read_servo_config() -&gt; dict\n</code></pre> <p>Read and return the servo configuration from the <code>servo_config.yml</code> file.</p>"},{"location":"Api%20Docs/commons/servo_config_file_schema/","title":"servo_config_file_schema","text":""},{"location":"Api%20Docs/commons/servo_config_file_schema/#basic_botcommonsservo_config_file_schema","title":"basic_bot.commons.servo_config_file_schema","text":"<p>This module contains the schema for the configuration file that the servo services read.</p> <p>It can be used with jsonvalidate to validate the configuration file.</p>"},{"location":"Api%20Docs/commons/servo_pca9685/","title":"servo_pca9685","text":""},{"location":"Api%20Docs/commons/servo_pca9685/#basic_botcommonsservo_pca9685","title":"basic_bot.commons.servo_pca9685","text":""},{"location":"Api%20Docs/commons/servo_pca9685/#servo-objects","title":"Servo Objects","text":"<pre><code>class Servo()\n</code></pre> <p>This class controls a single servo motor using the Adafruit PCA9685 servo controller. It uses a background thread to move the motor to a destination angle at a give fraction of the motor's range and thereby control the speed.</p> <p>It requires the following Adafruit CircuitPython libraries to be installed: <pre><code>python -m pip install adafruit-blinka adafruit-circuitpython-pca9685 adafruit-circuitpython-motorkit\n</code></pre></p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#__init__","title":"__init__","text":"<pre><code>def __init__(servo_options: ServoOptions) -&gt; None\n</code></pre> <p>Constructor</p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#current_angle","title":"current_angle","text":"<pre><code>@property\ndef current_angle() -&gt; float\n</code></pre> <p>Returns the current angle of the servo motor</p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#step_delay","title":"step_delay","text":"<pre><code>@property\ndef step_delay() -&gt; float\n</code></pre> <p>Returns the current step delay</p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#step_delay_1","title":"step_delay","text":"<pre><code>@step_delay.setter\ndef step_delay(value: float) -&gt; None\n</code></pre> <p>Sets the step delay</p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#move_to","title":"move_to","text":"<pre><code>def move_to(angle: float) -&gt; None\n</code></pre> <p>Move the servo to the specified angle</p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#pause","title":"pause","text":"<pre><code>def pause() -&gt; None\n</code></pre> <p>Pause the servo movement</p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#resume","title":"resume","text":"<pre><code>def resume() -&gt; None\n</code></pre> <p>Resume the servo movement</p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#stop_thread","title":"stop_thread","text":"<pre><code>def stop_thread() -&gt; None\n</code></pre> <p>Stop the servo movement thread</p> <p></p>"},{"location":"Api%20Docs/commons/servo_pca9685/#wait_for_motor_stopped","title":"wait_for_motor_stopped","text":"<pre><code>def wait_for_motor_stopped() -&gt; None\n</code></pre> <p>Wait for the motor to stop moving</p>"},{"location":"Api%20Docs/commons/tflite_detect/","title":"tflite_detect","text":""},{"location":"Api%20Docs/commons/tflite_detect/#basic_botcommonstflite_detect","title":"basic_bot.commons.tflite_detect","text":""},{"location":"Api%20Docs/commons/tflite_detect/#tflitedetect-objects","title":"TFLiteDetect Objects","text":"<pre><code>class TFLiteDetect()\n</code></pre> <p>This class provides object detection using Tensor Flow Lite.</p> <p></p>"},{"location":"Api%20Docs/commons/tflite_detect/#__init__","title":"__init__","text":"<pre><code>def __init__(model: Optional[str] = None,\n             use_coral_tpu: Optional[bool] = None) -&gt; None\n</code></pre> <p>Constructor</p> <p>Arguments:</p> <ul> <li>model: the path to the tflite model file</li> <li>use_coral_tpu: whether to use the Coral TPU for inference</li> </ul> <p></p>"},{"location":"Api%20Docs/commons/tflite_detect/#get_prediction","title":"get_prediction","text":"<pre><code>def get_prediction(img: Any) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Given an image array, return a list of detected objects.</p> <p>Each detected object is a dictionary with the following:</p> <ul> <li>bounding_box: [x1, y1, x2, y2]</li> <li>classification: string</li> <li>confidence: float (0-1)</li> </ul>"},{"location":"Api%20Docs/commons/vid_utils/","title":"vid_utils","text":""},{"location":"Api%20Docs/commons/vid_utils/#basic_botcommonsvid_utils","title":"basic_bot.commons.vid_utils","text":"<p>This module contains utility functions for working with OpenCV.</p> <p></p>"},{"location":"Api%20Docs/commons/vid_utils/#record_video","title":"record_video","text":"<pre><code>def record_video(camera: Camera, seconds: float) -&gt; str\n</code></pre> <p>Record a video to BB_VIDEO_PATH for a specified number of seconds.</p> <p>Calling this function will save an MP4 video file to the BB_VIDEO_PATH directory. The filename is the current date and time in the format <code>YYYYMMDD-HHMMSS.mp4</code>.</p> <p>It will also save the first frame of the video as a JPEG image in the same directory named <code>YYYYMMDD-HHMMSS.jpg</code>.</p> <p></p>"},{"location":"Api%20Docs/commons/vid_utils/#convert_video_to_h264","title":"convert_video_to_h264","text":"<pre><code>def convert_video_to_h264(video_file_in: str, video_file_out: str) -&gt; None\n</code></pre> <p>Sse system installed ffmpeg to convert video to h264.  This was needed to make the mp4 video captured using openCV compatible with web browsers.</p> <p>TODO: It would be nice to do this directly with cv2 VideoWriter, but I could not get it to work.  I think it is running into the same issue described here in StackOverflow The solution descibed however (use apt-get to install opencv python instead of pip) probably causes other issues :/.</p> <p></p>"},{"location":"Api%20Docs/commons/vid_utils/#get_recorded_videos","title":"get_recorded_videos","text":"<pre><code>def get_recorded_videos() -&gt; list[str]\n</code></pre> <p>Get a list of recorded videos in the BB_VIDEO_PATH directory.</p>"},{"location":"Api%20Docs/commons/web_utils/","title":"web_utils","text":""},{"location":"Api%20Docs/commons/web_utils/#basic_botcommonsweb_utils","title":"basic_bot.commons.web_utils","text":"<p>Utility functions for handling Flask web responses and requests.</p> <p></p>"},{"location":"Api%20Docs/commons/web_utils/#json_response","title":"json_response","text":"<pre><code>def json_response(app, data)\n</code></pre> <p>Return a JSON response.</p> <p></p>"},{"location":"Api%20Docs/commons/web_utils/#respond_ok","title":"respond_ok","text":"<pre><code>def respond_ok(app, data=None)\n</code></pre> <p>Return a JSON response with status ok.</p> <p></p>"},{"location":"Api%20Docs/commons/web_utils/#respond_not_ok","title":"respond_not_ok","text":"<pre><code>def respond_not_ok(app, status, data)\n</code></pre> <p>Return a JSON response with status not ok.</p>"},{"location":"Api%20Docs/debug/","title":"basic_bot.debug package","text":"<p>This package is a collection of individual test programs meant to validate the correct setup and installation of 3rd party packages like opencv and tensorflow.</p> <p>You can run them using the -m switch to python from the command line on either your workstation or your robot's on board computer.  Ex:</p> <pre><code># tests that opencv is working and outputs an \"opencv_test_output.jpg\"\npython -m basic_bot.debug.test_opencv_img\n</code></pre>"},{"location":"Api%20Docs/debug/test_ai-edge-litert/","title":"test_ai-edge-litert","text":""},{"location":"Api%20Docs/debug/test_ai-edge-litert/#basic_botdebugtest_ai-edge-litert","title":"basic_bot.debug.test_ai-edge-litert","text":"<p>This diagnostic script will test the installation and setup of the Google ai-edge-litert</p> <p>It uses opencv to open the test image file and then runs the model inference on the image.</p> <p>Needs to have ai-edge-litert from Google installed: <pre><code>python -m pip install ai-edge-litert\n</code></pre> Unfortunately the package will not install on Macs or Windows.  It will install on Linux and Raspberry Pi (arm64).</p> <p><code>ai-edge-litert</code> is suppose to be the future replacement of the tflite_runtime package.</p> <p>Usage: <pre><code>python -m basic_bot.debug.test_ai-edge-litert\n</code></pre></p> <p></p>"},{"location":"Api%20Docs/debug/test_ai-edge-litert/#editable_img","title":"editable_img","text":"<p>type: ignore</p>"},{"location":"Api%20Docs/debug/test_opencv_capture/","title":"test_opencv_capture","text":""},{"location":"Api%20Docs/debug/test_opencv_capture/#basic_botdebugtest_opencv_capture","title":"basic_bot.debug.test_opencv_capture","text":"<p>This diagnostic script will test the installation of open cv, and its ability to capture a 10sec video from a camera and save it to a file.</p> <p>usage: <pre><code>   python -m basic_bot.debug.test_opencv_capture 0\n</code></pre> where 0 (default; optional) is the video channel to use.</p> <p>from https://www.geeksforgeeks.org/saving-a-video-using-opencv/</p>"},{"location":"Api%20Docs/debug/test_opencv_img/","title":"test_opencv_img","text":""},{"location":"Api%20Docs/debug/test_opencv_img/#basic_botdebugtest_opencv_img","title":"basic_bot.debug.test_opencv_img","text":"<p>This diagnostic script will test the installation of open cv, the ability to read an image from a file, the ability to add text to the image, and the ability to save an image to a file.</p> <p>usage: <pre><code>python -m basic_bot.debug.test_opencv_img\n</code></pre></p>"},{"location":"Api%20Docs/debug/test_picam2_ffmpeg_capture/","title":"test_picam2_ffmpeg_capture","text":""},{"location":"Api%20Docs/debug/test_picam2_ffmpeg_capture/#basic_botdebugtest_picam2_ffmpeg_capture","title":"basic_bot.debug.test_picam2_ffmpeg_capture","text":"<p>This diagnostic script will test the installation of picamera2, and its ability to capture a 10sec video from a camera and save it to a file. Using ffmpeg to save the video.</p> <p>usage: <pre><code>python -m basic_bot.debug.test_picam2_ffmpeg_capture 0\n</code></pre></p> <p>Sourced from: https://github.com/raspberrypi/picamera2/blob/main/examples/mp4_capture.py</p>"},{"location":"Api%20Docs/debug/test_picam2_opencv_capture/","title":"test_picam2_opencv_capture","text":""},{"location":"Api%20Docs/debug/test_picam2_opencv_capture/#basic_botdebugtest_picam2_opencv_capture","title":"basic_bot.debug.test_picam2_opencv_capture","text":"<p>This diagnostic script will test the installation of picamera2, and its ability to capture a 10sec video from a camera and save it to a file. Using cv2 to save the video.</p> <p>usage: <pre><code>python -m basic_bot.debug.test_picam2_opencv_capture\n</code></pre></p>"},{"location":"Api%20Docs/debug/test_tflite_runtime/","title":"test_tflite_runtime","text":""},{"location":"Api%20Docs/debug/test_tflite_runtime/#basic_botdebugtest_tflite_runtime","title":"basic_bot.debug.test_tflite_runtime","text":"<p>Tests that the TensorFlow Lite runtime is installed and working.</p> <p>Usage: <pre><code>python -m basic_bot.debug.test_tflite_runtime\n</code></pre></p> <p>Must have TensorFlow Lite installed.  From your Python virtual environment: <pre><code>python -m pip install tflite-runtime\n</code></pre> The above may fail on mac (apple silicon) and windows.  It should work on Ubuntu Linux and Raspberry Pi.</p> <p>src: https://www.hackster.io/news/benchmarking-tensorflow-and-tensorflow-lite-on-raspberry-pi-5-b9156d58a6a2 https://github.com/aallan/benchmarking-ml-on-the-edge/blob/98467e058732a6626b6fb382980477b2121b34d2/benchmark_tf.py#L115</p>"},{"location":"Api%20Docs/scripts/","title":"basic_bot scripts","text":"<p>The scripts in this directory are used to manage your basic_bot project.  They are installed in the path by pip install of basic_bot.</p> <p>bb_create - create a new basic_bot project.  See also Getting Started</p> <p>bb_start - starts all of the services in the current directory's <code>basic_bot.yml</code> file.</p> <p>bb_stop - stops all of the services in the current directory's <code>basic_bot.yml</code> file.</p> <p>bb_ps - list all processes started by <code>bb_start</code> from anywhere.</p> <p>bb_killall - kill all processes started by <code>bb_start</code> from anywhere.</p>"},{"location":"Api%20Docs/scripts/bb_create/","title":"bb_create","text":""},{"location":"Api%20Docs/scripts/bb_create/#basic_botbb_create","title":"basic_bot.bb_create","text":"<p>Usage: <pre><code>    bb_create &lt;new_project_directory_name&gt;\n</code></pre></p> <p>Description:     Create a new robot project directory with the given name.  The new project     will contain a basic bot project structure with some example code and     scripts to get you started.  The new project will be created in the current     working directory.</p>"},{"location":"Api%20Docs/scripts/bb_killall/","title":"bb_killall","text":""},{"location":"Api%20Docs/scripts/bb_killall/#basic_botbb_killall","title":"basic_bot.bb_killall","text":"<p>Finds all processes started by bb_start and kills them with signal 15. It also removes all files in the local pids directory if they exist.</p> <p>This script differs from <code>bb_stop</code> in that it works more like the *nix <code>killall</code> command, which sends a signal to all processes that match a query.   It does not discriminate based on any <code>basic_bot.yml</code> file like <code>bb_stop</code> does.</p> <p>See also: <code>bb_ps</code> script.</p> <p>bb_killall is a script installed in the path by pip install of basic_bot.</p> <p>usage: <pre><code>bb_killall\n</code></pre></p>"},{"location":"Api%20Docs/scripts/bb_ps/","title":"bb_ps","text":""},{"location":"Api%20Docs/scripts/bb_ps/#basic_botbb_ps","title":"basic_bot.bb_ps","text":"<p>Finds and lists all processes that were started by bb_start.</p> <p>When <code>bb_start</code> is used to start a process, it adds a <code>via=bb_start</code> to the command line. You can also see the list of processes started by <code>bb_start</code> by using <code>ps -ef | grep \"via=bb_start\"</code> from the terminal.</p> <p>bb_ps is a script installed in the path by pip install of basic_bot.</p> <p>usage: <pre><code>bb_ps\n</code></pre></p>"},{"location":"Api%20Docs/scripts/bb_start/","title":"bb_start","text":""},{"location":"Api%20Docs/scripts/bb_start/#basic_botbb_start","title":"basic_bot.bb_start","text":"<p>bb_start is a script to start services in the background. It reads a list of services from the <code>basic_bot.yml</code> file unless a <code>--file filename</code> is specified on the command line.</p> <p>For each service in the config file, bb_start:</p> <ul> <li>starts the service in the background detached from the terminal.</li> <li>writes the PID of the service to a file in the 'pids' directory. The PID file is used by the <code>bb_stop</code>.</li> <li>rotates log files for each service.  If the log file already exists, it is renamed to <code>service_name.log.1</code> and the previous is <code>...log.2</code>, etc.</li> <li><code>stdout</code> and <code>stderr</code> are redirected to a log files in the 'logs' directory with the name of the service from the configuration file.</li> </ul> <p>For more information on usage: <pre><code>bb_start --help\n</code></pre></p>"},{"location":"Api%20Docs/scripts/bb_stop/","title":"bb_stop","text":""},{"location":"Api%20Docs/scripts/bb_stop/#basic_botbb_stop","title":"basic_bot.bb_stop","text":"<p><code>bb_stop</code> is a script to stop basic_bot services started with <code>bb_start</code></p> <p>The services to stop are read from the <code>basic_bot.yml</code> file unless a <code>--file filename</code> is specified on the command line.</p> <p>For more information on usage: <pre><code>bb_stop --help\n</code></pre></p>"},{"location":"Api%20Docs/services/","title":"basic_bot services","text":"<p>These are the services that come with basic_bot.  Many are hardware dependent.  Some require additional drivers installed.  See documentation for each service to learn more about what it requires and provides.</p> <p>central_hub is the only service that must be started.  It provides an under 5ms roundtrip latency pub/sub service that the other services communicate state through.</p> <p>system_stats is an optional service that publishes information about utilization of the system, like memory used, cpu used and temperatures</p> <p>web_server is an optional service that serves a web UI from your robot.</p> <p>vision provides live video streaming and in-frame object detection and classification.</p> <p>motor_control_2w provides motor control of a 2 wheel or track drive mobile robot.</p> <p>servo_control provides control of up to 16 servos.</p>"},{"location":"Api%20Docs/services/central_hub/","title":"central_hub","text":""},{"location":"Api%20Docs/services/central_hub/#basic_botservicescentral_hub","title":"basic_bot.services.central_hub","text":"<p>Provides an ultra-light pub/sub service with &lt; 5ms latecy over websockets.  The state of published keys is maintained in memory.</p> <p>This python process is meant to be run as a service by basic_bot.</p> <p>You can also run it in the foreground for debugging purposes.  ex: <pre><code>python -m basic_bot.services.central_hub\n</code></pre></p> <p>Central hub is also the publisher of several state keys: <pre><code>{\n    \"hub_stats\": {\n        \"state_updates_recv\": 0\n    },\n    \"subsystem_stats\": {}\n}\n</code></pre></p>"},{"location":"Api%20Docs/services/central_hub/#messages","title":"Messages","text":"<p>All data sent over the websocket to and from central_hub is in json and has the format: <pre><code>{\n     \"type\": \"string\",\n     \"data\": { ... }\n}\n</code></pre> Where <code>data</code> is optional and specific to the type of message. The following messages are supported by <code>central-hub</code>:</p>"},{"location":"Api%20Docs/services/central_hub/#getstate","title":"getState","text":"<p>example json:</p> <p><pre><code>{\n  \"type\": \"getState\"\n  \"data\": [\"keyName\", \"keyName\"]\n}\n</code></pre> Causes <code>central-hub</code> to send the requested state via message type = \"state\" to the requesting client socket.</p> <p><code>data</code> is optional, if specified, should be array of key names to retrieve. If omitted, all keys (complete state) is sent.</p>"},{"location":"Api%20Docs/services/central_hub/#identity","title":"identity","text":"<p>example json:</p> <pre><code>{\n  \"type\": \"identity\",\n  \"data\": \"My subsystem name\"\n}\n</code></pre> <p>Causes <code>central-hub</code> to update <code>subsystems_stats</code> key of the shared state and send an \"iseeu\" message back to client socket with the IP address that it sees the client.</p>"},{"location":"Api%20Docs/services/central_hub/#subscribestate","title":"subscribeState","text":"<p>example json:</p> <pre><code>{\n  \"type\": \"subscribeState\",\n  \"data\": [\"system_stats\", \"set_angles\"]\n}\n</code></pre> <p>Causes <code>central-hub</code> to add the client socket to the subscribers for each of the state keys provided. Client will start receiving \"stateUpdate\" messages when those keys are changed. The client may also send <code>\"data\": \"*\"</code> which will subscribe it to all keys like the web UI does.</p>"},{"location":"Api%20Docs/services/central_hub/#updatestate","title":"updateState","text":"<p>example json:</p> <pre><code>{\n  \"type\": \"updateState\",\n  \"data\": {\n    \"set_angles\": [127.4, 66.4, 90, 90, 0],\n    \"velocity_factor\": 1.5\n  }\n}\n</code></pre> <p>This message causes <code>central-hub</code> merge the receive state and the shared state and send <code>stateUpdate</code> messages to any subscribers. Note that the message sent by clients (type: \"updateState\") is a different type than the message sent to clients** (type: \"stateUpdate\").</p> <p>As the example above shows, it is possible to update multiple state keys at once, but most subsystems only ever update one top level key.</p> <p>The data received must be the full data for that key. <code>central-hub</code> will replace that top level key with the data received.</p>"},{"location":"Api%20Docs/services/motor_control_2w/","title":"motor_control_2w","text":""},{"location":"Api%20Docs/services/motor_control_2w/#basic_botservicesmotor_control_2w","title":"basic_bot.services.motor_control_2w","text":"<p>Provides motor control for a 2-wheel drive robot via the central_hub key: \"throttles\".</p> <p>Sending a state update to the central_hub, for example: <pre><code>{\n    \"throttles\": {\n        \"left\": 0.5,\n        \"right\": 0.5\n    }\n}\n</code></pre> ...would set the left and right throttles to half ahead.  You can also set the throttles to negative values &gt;= -1 to go in reverse.</p> <p>The motor control will send a state update back to the central_hub with the current state of the motors, for example: <pre><code>{\n    \"motors\": {\n        \"left\": 0.5,\n        \"right\": 0.5\n    }\n}\n</code></pre></p>"},{"location":"Api%20Docs/services/servo_control/","title":"servo_control","text":""},{"location":"Api%20Docs/services/servo_control/#basic_botservicesservo_control","title":"basic_bot.services.servo_control","text":"<p>Provides a service to control the servo motors using the PCA9685 PWM driver.</p> <p>On startup, this service will look for a file named <code>servo_config.yml</code> in the directory where the service was started which should always be the root project dir of your basic_bot project. This file should contain a list of servos with the following format:</p> <pre><code>servos:\n  - name: servo_name\n    channel: 0\n    motor_range: 180\n    min_angle: 0\n    max_angle: 180\n    min_pulse: 500\n    max_pulse: 2500\n</code></pre> <p>If you update the <code>servo_config.yml</code> file, you will need to restart the service.</p> <p>The 'name' and 'channel' are required for each servo.  The example above shows the default values for the other parameters.</p> <ul> <li><code>name</code> (required) is the unique name of the servo.</li> <li><code>channel</code> (required) is the channel on the PCA9685 board that the servo is connected to.</li> <li><code>motor_range</code> is the total manufacturer's range of the servo in degrees.</li> <li><code>min_angle</code> and <code>max_angle</code> are the minimum and maximum angles that the servo should    be constrained.</li> <li><code>min_pulse</code> and <code>max_pulse</code> are the minimum and maximum pulse widths in microseconds    that the servo will accept as specified by the manufacturer.</li> </ul> <p>The service listens for messages on the central_hub key: \"servo_angles\". The message <code>data</code> should be a dictionary with keys that are the servo <code>names</code> from <code>servo_config.yml</code> and values that are the desired angle in degrees.</p> <pre><code>{\n    \"servo_angles\": {\n        \"servo_name\": 90\n    }\n}\n</code></pre> <p>The service will send a state update back to the central_hub with the current state of the servos using the key \"servo_current_angles\", for example: <pre><code>{\n    \"servo_actual_angles\": {\n        \"servo_name\": 90\n    }\n}\n</code></pre></p> <p>The service will also provide the current servo config as read from <code>servo_config.yml</code> at service startup using the key \"servo_config\", for example: <pre><code>{\n    \"servo_config\": {\n        \"servos\": [\n            {\n                \"name\": \"servo_name\",\n                \"channel\": 0,\n                \"motor_range\": 180,\n                \"min_angle\": 0,\n                \"max_angle\": 180,\n                \"min_pulse\": 500,\n                \"max_pulse\": 2500\n            }\n        ]\n    }\n}\n</code></pre></p> <p></p>"},{"location":"Api%20Docs/services/servo_control/#angle_update_frequency","title":"ANGLE_UPDATE_FREQUENCY","text":"<p>seconds = 10Hz</p>"},{"location":"Api%20Docs/services/system_stats/","title":"system_stats","text":""},{"location":"Api%20Docs/services/system_stats/#basic_botservicessystem_stats","title":"basic_bot.services.system_stats","text":"<p>This optional services provides system usage statistics to the central hub under the <code>system_stats</code> key.</p> <p>The information is sampled using the psutil library and includes:</p> <pre><code>{\n    \"system_stats\": {\n        \"cpu_util\": psutil.cpu_percent(),\n        \"cpu_temp\": cpu_temp,\n        \"ram_util\": psutil.virtual_memory()[2],\n        \"hostname\": socket.gethostname(),\n    }\n}\n</code></pre> <p>Add to your basic_bot.yml file to enable this service: <pre><code>services:\n  - name: \"system_stats\"\n    run: \"python -m basic_bot.services.system_stats\"\n</code></pre></p>"},{"location":"Api%20Docs/services/vision/","title":"vision","text":""},{"location":"Api%20Docs/services/vision/#basic_botservicesvision","title":"basic_bot.services.vision","text":"<p>Provide image feed and object recognition based on open-cv for the video capture input.  This service will provide a list of objects and their bounding boxes in the image feed via central hub.</p>"},{"location":"Api%20Docs/services/vision/#video-feed","title":"Video Feed","text":"<p>A video feed is provided via http://:/video_feed that can be used as the <code>src</code> attribute to an HTML 'img' element. The image feed is a multipart jpeg stream (for now; TODO: reassess this). Assuming that the vision service is running on the same host machine as the browser client location, you can do something like:"},{"location":"Api%20Docs/services/vision/#object-recognition","title":"Object Recognition","text":"<p>The following data is provided to central_hub as fast as image capture and recognition can be done:</p> <p>The [x1, y1, x2, y2] bounding box above is actually sent as the numeric values of the bounding box in the image.</p>"},{"location":"Api%20Docs/services/vision/#video-recording","title":"Video Recording","text":"<p>To use the video recording feature, you must have the <code>ffmpeg</code> command and the libx264-dev library installed on the host machine:</p> <p>The video feed can be recorded using the <code>record_video</code> REST API. The video is recorded in the BB_VIDEO_PATH directory. Example:</p> <p>where - <code>localhost</code> is replaced by the IP address of the host running the vision service. - <code>5801</code> is the port the vision service is running on (default).  See <code>BB_VISION_PORT</code> in the configuration docs.</p> <p>Filename of the saved file is the current date and time in the format <code>YYYYMMDD-HHMMSS.mp4</code>.</p>"},{"location":"Api%20Docs/services/vision/#origin","title":"Origin","text":"<p>Some of this code was originally pilfered from https://github.com/adeept/Adeept_RaspTank/blob/a6c45e8cc7df620ad8977845eda2b839647d5a83/server/app.py</p> <p>Which looks like it was in turn pilfered from https://blog.miguelgrinberg.com/post/flask-video-streaming-revisited</p> <p>Thank you, @adeept and @miguelgrinberg! <pre><code>&lt;img src=\"http://localhost:5001/video_feed\" /&gt;\n</code></pre> <pre><code>{\n    \"recognition\": [\n        {\n            \"bounding_box\": [x1, y1, x2, y2],\n            \"classification\": \"person\",\n            \"confidence\": 0.99\n        },\n        {\n            \"bounding_box\": [x1, y1, x2, y2],\n            \"classification\": \"dog\",\n            \"confidence\": 0.75\n        }\n    ]\n}\n</code></pre> <pre><code>sudo apt install -y ffmpeg libx264-dev\n</code></pre> <pre><code>curl http://localhost:5801/record_video\n</code></pre></p> <p></p>"},{"location":"Api%20Docs/services/vision/#camera","title":"camera","text":"<p>type: ignore</p> <p></p>"},{"location":"Api%20Docs/services/vision/#gen_rgb_video","title":"gen_rgb_video","text":"<pre><code>def gen_rgb_video(camera: BaseCamera) -&gt; Generator[bytes, None, None]\n</code></pre> <p>Video streaming generator function.</p> <p></p>"},{"location":"Api%20Docs/services/vision/#video_feed","title":"video_feed","text":"<pre><code>@app.route(\"/video_feed\")\ndef video_feed() -&gt; Response\n</code></pre> <p>Video streaming route. Put this in the src attribute of an img tag.</p> <p></p>"},{"location":"Api%20Docs/services/vision/#send_stats","title":"send_stats","text":"<pre><code>@app.route(\"/stats\")\ndef send_stats() -&gt; Response\n</code></pre> <p>Return the FPS and other stats of the vision service.</p> <p></p>"},{"location":"Api%20Docs/services/vision/#pause_recognition","title":"pause_recognition","text":"<pre><code>@app.route(\"/pause_recognition\")\ndef pause_recognition() -&gt; Response\n</code></pre> <p>Use a GET request to pause the recognition provider.</p> <p></p>"},{"location":"Api%20Docs/services/vision/#resume_recognition","title":"resume_recognition","text":"<pre><code>@app.route(\"/resume_recognition\")\ndef resume_recognition() -&gt; Response\n</code></pre> <p>Use a GET request to resume the recognition provider.</p> <p></p>"},{"location":"Api%20Docs/services/vision/#record_video","title":"record_video","text":"<pre><code>@app.route(\"/record_video\")\ndef record_video() -&gt; Response\n</code></pre> <p>Record the video feed to a file.</p> <p></p>"},{"location":"Api%20Docs/services/vision/#recorded_video","title":"recorded_video","text":"<pre><code>@app.route(\"/recorded_video\")\ndef recorded_video() -&gt; Response\n</code></pre> <p>Returns json array of string filenames (without extension) of all recorded video files.   The filenames can be used to download the using a url like <code>http://&lt;ip&gt;:&lt;port&gt;/recorded_video/&lt;filename&gt;.mp4</code> or <code>http://&lt;ip&gt;:&lt;port&gt;/recorded_video/&lt;filename&gt;.jpg</code> for the thumbnail image.</p> <p></p>"},{"location":"Api%20Docs/services/vision/#send_static_js","title":"send_static_js","text":"<pre><code>@app.route(\"/recorded_video/&lt;filename&gt;\")\ndef send_static_js(filename: str) -&gt; Response\n</code></pre> <p>Sends a recorded video file.</p>"},{"location":"Api%20Docs/services/web_server/","title":"web_server","text":""},{"location":"Api%20Docs/services/web_server/#basic_botservicesweb_server","title":"basic_bot.services.web_server","text":"<p>Simple http server for serving the react web app from build dir</p>"},{"location":"Api%20Docs/test_helpers/camera_mock/","title":"camera_mock","text":""},{"location":"Api%20Docs/test_helpers/camera_mock/#basic_bottest_helperscamera_mock","title":"basic_bot.test_helpers.camera_mock","text":""},{"location":"Api%20Docs/test_helpers/camera_mock/#camera-objects","title":"Camera Objects","text":"<pre><code>class Camera(BaseCamera)\n</code></pre> <p>This class implements a mock of BaseCamera interface using a set of static images. The images are loaded from the pet_images and not_pet_images test data. The mock camera will provide 50% pet images and 50% non pet image frames at 60 frames per second.</p> <p>When running in BB_ENV=test, the vision service will use this mock camera in place of camera configured by BB_CAMERA_MODULE.</p> <p></p>"},{"location":"Api%20Docs/test_helpers/camera_mock/#frames","title":"frames","text":"<pre><code>@staticmethod\ndef frames() -&gt; Generator[bytes, None, None]\n</code></pre> <p>Generator function that yields frames from the camera. Required by BaseCamera</p>"},{"location":"Api%20Docs/test_helpers/central_hub/","title":"central_hub","text":""},{"location":"Api%20Docs/test_helpers/central_hub/#basic_bottest_helperscentral_hub","title":"basic_bot.test_helpers.central_hub","text":""},{"location":"Api%20Docs/test_helpers/central_hub/#connect","title":"connect","text":"<pre><code>def connect(identity: Optional[str] = None) -&gt; websocket.WebSocket\n</code></pre> <p>connect to central hub and return a websocket (websocket-client lib)</p> <p></p>"},{"location":"Api%20Docs/test_helpers/central_hub/#send","title":"send","text":"<pre><code>def send(ws: websocket.WebSocket, dict: Dict[str, Any]) -&gt; None\n</code></pre> <p>send dictionary as json to central hub</p>"},{"location":"Api%20Docs/test_helpers/constants/","title":"constants","text":""},{"location":"Api%20Docs/test_helpers/constants/#basic_bottest_helpersconstants","title":"basic_bot.test_helpers.constants","text":""},{"location":"Api%20Docs/test_helpers/skip_unless_tflite_runtime/","title":"skip_unless_tflite_runtime","text":""},{"location":"Api%20Docs/test_helpers/skip_unless_tflite_runtime/#basic_bottest_helpersskip_unless_tflite_runtime","title":"basic_bot.test_helpers.skip_unless_tflite_runtime","text":""},{"location":"Api%20Docs/test_helpers/start_stop/","title":"start_stop","text":""},{"location":"Api%20Docs/test_helpers/start_stop/#basic_bottest_helpersstart_stop","title":"basic_bot.test_helpers.start_stop","text":""},{"location":"Api%20Docs/test_helpers/start_stop/#start_service","title":"start_service","text":"<pre><code>def start_service(service_name: str,\n                  run_cmd: str,\n                  env: Dict[str, str] = {}) -&gt; None\n</code></pre> <p>Starts requested service using bb_start command.  This is useful for integration and e2e tests.</p> <p>BB_ENV=test is forced to ensure that the services are started in the test environment which means motor control, vision camera, and other hardware specific modules are mocked.</p> <p>Arguments:</p> <p>service_name = the name of the service. \"test_\" is prepended to the   service name to create pid and and log files that are distinct from   the services which may be running for development.</p> <p>run_cmd = the command to start the service.</p> <p></p>"},{"location":"Api%20Docs/test_helpers/start_stop/#stop_service","title":"stop_service","text":"<pre><code>def stop_service(service_name) -&gt; None\n</code></pre> <p>stops the requested service using bb_stop command.</p> <p>Arguments:</p> <ul> <li><code>service_name</code> - the name of the service to stop as it was passed to   start_service function above</li> </ul>"},{"location":"Configuration/","title":"Configuring Basic Bot","text":"<p>Basic Bot can be configured via environment variables that can be set in the <code>basic_bot.yml</code> file or externally.</p>"},{"location":"Configuration/#configuration-file","title":"Configuration File","text":"<p>Config File Schema herein this doc directory for more information about the file that is used by <code>bb_start</code> to start services.</p>"},{"location":"Configuration/#all-the-env-vars","title":"All the env vars","text":"<p>Envivonment Variables in this docs directory has documentation for all of the configurable constants that can be set via environment variables.</p>"},{"location":"Configuration/Config%20File%20Schema/","title":"Config File Schema","text":"<pre><code>#\n#  Development Note:  If you change the name of this file, or\n#   it's relative path, you will need to update the /build_docs.py script\n#\n\nconfig_file_schema = {\n    \"type\": \"object\",\n    \"required\": [\"bot_name\", \"version\", \"services\"],\n    \"properties\": {\n        #\n        # The name of the bot.  This is used for config data and made available\n        # via the central hub to all services. TODO: not done yet\n        \"bot_name\": {\"type\": \"string\"},\n        #\n        # Version of basic_bot that the configuration file is compatible with.\n        \"version\": {\"type\": \"string\"},\n        #\n        # Environment variables that are set for all services started unless\n        # overridden by the service `env' property.\n        \"env\": {\"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}},\n        #\n        # Environment variables that are set for all services started unless\n        # overridden by the service `env' property. These are merged with the\n        # `env` property\n        \"test_env\": {\"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}},\n        \"production_env\": {\n            \"type\": \"object\",\n            \"additionalProperties\": {\"type\": \"string\"},\n        },\n        \"development_env\": {\n            \"type\": \"object\",\n            \"additionalProperties\": {\"type\": \"string\"},\n        },\n        #\n        # List of services to start.  Each service is started in the background\n        # as a detached process.\n        \"services\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"name\", \"run\"],\n                \"properties\": {\n                    #\n                    # Must have unique name for each service.  `name`` is also\n                    # used to create the pid file and log file.\n                    \"name\": {\"type\": \"string\"},\n                    #\n                    # The command to run to start the service.  This command can be\n                    # any valid shell command.  The command is run in the current\n                    # working directory.\n                    \"run\": {\"type\": \"string\"},\n                    #\n                    # log_file and pid_file are optional file paths to write the\n                    # logs and pid for this service.  `bb_stop` uses the pid file\n                    # to stop the service. If not provided, the log and pid are\n                    # written to the logs and pids directories with the name of the\n                    # service.\n                    \"log_file\": {\"type\": \"string\"},\n                    \"pid_file\": {\"type\": \"string\"},\n                    #\n                    # Environment variables that are set for this service. These\n                    # are merged with the `env` properties for the overall\n                    # configuration above.  These are the penultimate environmen.\n                    \"env\": {\n                        \"type\": \"object\",\n                        \"additionalProperties\": {\"type\": \"string\"},\n                    },\n                    #\n                    # Environment variables that are set for this service in the\n                    # test, production and development  environments.\n                    # These are merged with the `env`s above and are the\n                    # ultimate environment variables.\n                    \"test_env\": {\n                        \"type\": \"object\",\n                        \"additionalProperties\": {\"type\": \"string\"},\n                    },\n                    \"production_env\": {\n                        \"type\": \"object\",\n                        \"additionalProperties\": {\"type\": \"string\"},\n                    },\n                    \"development_env\": {\n                        \"type\": \"object\",\n                        \"additionalProperties\": {\"type\": \"string\"},\n                    },\n                },\n            },\n        },\n    },\n}\n</code></pre>"},{"location":"Configuration/Environment%20Variables/","title":"Environment Variables","text":"<pre><code>#\n#  Development Note:  If you change the name of the this file, or\n#   it's relative path, you will need to update the /build_docs.py script\n#\n\n\nimport basic_bot.commons.env as env\n\nBB_ENV = env.env_string(\"BB_ENV\", \"development\")\n\"\"\"\nDefault: \"development\"\n\nTest runner should set this to \"test\".\n\nSet this to \"production\" on the onboard computer to run the bot in production\nmode.\n\"\"\"\n\nBB_LOG_ALL_MESSAGES = env.env_bool(\"BB_LOG_ALL_MESSAGES\", False)\n\"\"\"\n    Default: False\n\n    Set this to True to log all messages sent and received by services to\n    each of their respective log files.\n\"\"\"\n\n\nBB_HUB_PORT = hub_port = env.env_int(\"BB_HUB_PORT\", 5100)\n\"\"\"\n    Default: 5100\n\n    This is the websocket port that the central hub listens on.\n\"\"\"\n\n\n# Connect to central hub websocket\nBB_HUB_URI = f\"ws://127.0.0.1:{BB_HUB_PORT}/ws\"\n\"\"\"\n    Default: `f\"ws://127.0.0.1:{BB_HUB_PORT}/ws\"`\n\n\"\"\"\n\n\n# =============== Motor Control Service Constants\nBB_MOTOR_I2C_ADDRESS = env.env_int(\"BB_MOTOR_I2C_ADDRESS\", 0x60)\n\"\"\"\n    default: 0x60\n\n    Used by basic_bot.services.motor_control_2w to connect to the\n    i2c motor controller.\n\"\"\"\nBB_LEFT_MOTOR_CHANNEL = env.env_int(\"BB_LEFT_MOTOR_CHANNEL\", 1)\n\"\"\"\n    default: 1\n\"\"\"\nBB_RIGHT_MOTOR_CHANNEL = env.env_int(\"BB_RIGHT_MOTOR_CHANNEL\", 2)\n\"\"\"\n    default: 2\n\"\"\"\n\n# =============== Servo Service Constants\nBB_SERVO_CONFIG_FILE = env.env_string(\"BB_SERVO_CONFIG_FILE\", \"./servo_config.yml\")\n\"\"\"\n    See api docs for servo control services\n\"\"\"\n\n# =============== System Stats Service Constants\nBB_SYSTEM_STATS_SAMPLE_INTERVAL = env.env_float(\"BB_SYSTEM_STATS_SAMPLE_INTERVAL\", 1)\n\"\"\"\n    In seconds, the interval at which the system stats service samples the system\n    and publishes system stats to the central hub.\n\n    default: 1\n\"\"\"\n\n\n# =============== Vision service constants\ndefault_camera_module = (\n    \"basic_bot.test_helpers.camera_mock\"\n    if BB_ENV == \"test\"\n    else \"basic_bot.commons.camera_opencv\"\n)\n# see other supported camera modules in basic_bot.commons.camera_*\n# for example, `BB_CAMERA_MODULE=basic_bot.commons.camera_picamera`\nBB_CAMERA_MODULE = env.env_string(\"BB_CAMERA_MODULE\", default_camera_module)\n\"\"\"\n    default: \"basic_bot.commons.camera_opencv\" or\n        \"basic_bot.test_helpers.camera_mock\" when BB_ENV=test\n\n    When using a ribbon cable camera on a Raspberry Pi 4/5, set this to\n    \"basic_bot.commons.camera_picamera\".\n\"\"\"\n\nBB_CAMERA_CHANNEL = env.env_int(\"BB_CAMERA_CHANNEL\", 0)\n\"\"\"\n    default: 0\n\n    This is the camera channel to use. 0 is the default camera.\n\"\"\"\n\nBB_CAMERA_ROTATION = env.env_int(\"BB_CAMERA_ROTATION\", 0)\n\"\"\"\n    default: 0\n\n    This is the camera rotation in degrees. 0 is the default rotation.\n\"\"\"\n\nBB_CAMERA_FPS = env.env_int(\"BB_CAMERA_FPS\", 30)\n\"\"\"\n    default: 30\n\n    This is the camera setting frames per second.\n\"\"\"\n\nBB_VISION_WIDTH = env.env_int(\"BB_VISION_WIDTH\", 640)\n\"\"\"\n    default: 640\n\n    In pixels, this is the width of the camera frame to capture.\n\"\"\"\n\nBB_VISION_HEIGHT = env.env_int(\"BB_VISION_HEIGHT\", 480)\n\"\"\"\n    default: 480\n\n    In pixels, this is the height of the camera frame to capture.\n\n\"\"\"\n\nBB_VISION_FOV = 62\n\"\"\"\n    default: 62\n\n    This is the field of view of the camera in degrees.\n    Depends on camera; RPi v2 cam is 62deg.\n\"\"\"\n\nBB_OBJECT_DETECTION_THRESHOLD = env.env_float(\"BB_OBJECT_DETECTION_THRESHOLD\", 0.5)\n\"\"\"\n    default: 0.5\n\n    This is the object detection threshold percentage 0 - 1; higher = greater confidence.\n\"\"\"\n\n# to enable the Coral USB TPU, you must use the tflite_detector and set this to True\nBB_ENABLE_CORAL_TPU = env.env_bool(\"BB_ENABLE_CORAL_TPU\", False)\n\"\"\"\n    default: False\n\n    Set this to True to enable the Coral USB TPU for object detection.\n\"\"\"\n\nBB_TFLITE_MODEL = env.env_string(\n    \"BB_TFLITE_MODEL\", \"./models/tflite/ssd_mobilenet_v1_coco_quant_postprocess.tflite\"\n)\n\"\"\"\n    default: \"./models/tflite/ssd_mobilenet_v1_coco_quant_postprocess.tflite\"\n\n    Which model to use for object detection when BB_ENABLE_CORAL_TPU is false.\n    Default is the model from the coral site which is faster than the model\n    from the tensorflow hub\n\"\"\"\n\nBB_TFLITE_MODEL_CORAL = env.env_string(\n    \"BB_TFLITE_MODEL_CORAL\",\n    \"./models/tflite/ssd_mobilenet_v1_coco_quant_postprocess_edgetpu.tflite\",\n)\n\"\"\"\n    default: \"./models/tflite/ssd_mobilenet_v1_coco_quant_postprocess_edgetpu.tflite\"\n\n    Which model to use for object detection when BB_ENABLE_CORAL_TPU is true.\n\"\"\"\n\n# number of threads to use for tflite detection\nBB_TFLITE_THREADS = env.env_int(\"BB_TFLITE_THREADS\", 3)\n\"\"\"\n    default: 3\n\n    Number of threads to use for tflite detection.  Testing object detection\n    on a Rasberry PI 5, without any other CPU or memory pressure, 4 tflite threads\n    was only 1 fps faster (29.5fps) than 3 threads (28.6fps).  2 threads was 22fps.\n\n    Warning: Setting this too high can actually reduce the object detection  frame rate.\n    In the case of daphbot_due, which has a [pygame based onboard UI service](https://github.com/littlebee/daphbot-due/blob/main/src/onboard_ui_service.py)\n    that has a configurable render frame rate, the tflite detection running on 4 threads\n    was reduced to about 12 fps when the render frame rate was set to 30fps.\n\"\"\"\n\n# http port used by the vision service for video streaming\nport = 5802 if BB_ENV == \"test\" else 5801\nBB_VISION_PORT = env.env_int(\"BB_VISION_PORT\", port)\n\"\"\"\n    default: 5802 when BB_ENV=test; 5801 otherwise\n\n    This is the HTTP port that the vision service listens  on for video\n    streaming and REST api.\n\"\"\"\n\nBB_DISABLE_RECOGNITION_PROVIDER = env.env_bool(\"BB_DISABLE_RECOGNITION_PROVIDER\", False)\n\"\"\"\n    default: False\n\n    Set this to True to disable the recognition provider.\n\"\"\"\n\nBB_VIDEO_PATH = env.env_string(\"BB_VIDEO_PATH\", \"./recorded_video\")\n\"\"\"\n    default: \"./recorded_video\"\n\n    The path where the vision service saves recorded video.\n\"\"\"\n\n\n# # Logging all the env variables in test for debugging\n# if BB_ENV == \"test\":\n#     print(\"Environment variables:\")\n#     for name, value in os.environ.items():\n#         print(\"{0}: {1}\".format(name, value))\n</code></pre>"},{"location":"Installation%20Guides/setup_on_pi_bookworm/","title":"Setup basic_bot on a Raspberry Pi4 or Pi5 with Debian Bookworm","text":"<p>...for basic_bot, ovencv and tflite</p>"},{"location":"Installation%20Guides/setup_on_pi_bookworm/#flash-and-boot","title":"Flash and Boot","text":"<p>First you need to flash an image to a microcard reader using the Raspberry Pi installer.</p> <p>Debian Bookworm is the new Raspian Bullseye.  Good article on the differences. The rest of this guide is specific to Bookwork, which at the time of this update is the default OS used by the Raspberry Pi Imager.</p> <p>Here is a montage of the various settings I chose:</p> <p> </p> <p>Just answer yes to the remaining questions and ...</p> <p>}</p>"},{"location":"Installation%20Guides/setup_on_pi_bookworm/#ssh-into-the-pi","title":"SSH into the pi","text":"<pre><code>ssh pi5.local\n</code></pre> <p>Verify that Debian Bookwork was installed: <pre><code>bee@pi5:~ $ cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\nVERSION=\"12 (bookworm)\"\nVERSION_CODENAME=bookworm\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n</code></pre></p>"},{"location":"Installation%20Guides/setup_on_pi_bookworm/#update-and-upgrade-os","title":"Update and upgrade OS","text":"<p><pre><code>sudo apt update\nsudo apt full-upgrade\nsudo reboot\n</code></pre> SSH back into Raspberry Pi after reboot.</p> <p>Check versions: <pre><code>bee@pi5:~ $ python --version\nPython 3.11.2\nbee@pi5:~ $ python -m pip --version\npip 23.0.1 from /usr/lib/python3/dist-packages/pip (python 3.11)\n</code></pre></p>"},{"location":"Installation%20Guides/setup_on_pi_bookworm/#install-basic-bot","title":"Install Basic Bot","text":"<p>Follow the instructions in Getting Started for installing and running basic_bot.</p>"},{"location":"Installation%20Guides/setup_on_pi_bookworm/#use-picamera2-instead-of-opencv-if-using-ribbon-cable-camera","title":"Use picamera2 instead of opencv if using ribbon cable camera","text":"<p>As of Feb 7, 2025, OpenCV camera capture will NOT work on Debian Bullseye or Bookworm with a ribbon cable camera.</p> <p>You must either use a USB camera or use the <code>basic_bot.commons.camera_picamera</code> module.</p> <p>See the API docs for using camera_picamera for more information about how to use.</p>"},{"location":"Installation%20Guides/setup_on_pi_bullseye/","title":"Setup basic_bot on a Raspberry Pi4 with Raspian Bullseye","text":"<p>...for basic_bot, ovencv and tflite</p>"},{"location":"Installation%20Guides/setup_on_pi_bullseye/#flash-and-boot","title":"Flash and Boot","text":"<p>First you need to flash an image to a microcard reader using the Raspberry Pi installer.</p> <p>Debian Bookworm is the new Raspian Bullseye.  Good article on the differences. The rest of this guide is specific to Bullseye.</p> <p>If you are setting up a new Raspberry Pi4 or Pi5, you should probably use the latest OS (Bookworm) unless you have a real need to use Bullseye.  See also our guide for installing on Pi4 or Pi5 with Debian Bookwork.</p> <p>To validate basic bot, I used the Raspberry Installer to flash a micro SSD.  I selected Bullseye from the legacy OSs.</p> <p></p>"},{"location":"Installation%20Guides/setup_on_pi_bullseye/#ssh-into-the-pi","title":"SSH into the pi","text":"<pre><code>ssh pi4.local\n</code></pre> <p>Verify that Raspian/Debian bullseye was installed: <pre><code>bee@pi4:~ $ cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 11 (bullseye)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"11\"\nVERSION=\"11 (bullseye)\"\nVERSION_CODENAME=bullseye\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"```\n</code></pre></p>"},{"location":"Installation%20Guides/setup_on_pi_bullseye/#update-and-upgrade-os","title":"Update and upgrade OS","text":"<p><pre><code>sudo apt update\nsudo apt full-upgrade\nsudo reboot\n</code></pre> SSH back into Raspberry Pi after reboot.</p> <p>Check versions: <pre><code>bee@pi4:~ $ python --version\nPython 3.9.2\nbee@pi4:~ $ python -m pip --version\npip 20.3.4 from /usr/lib/python3/dist-packages/pip (python 3.9)\n</code></pre></p>"},{"location":"Installation%20Guides/setup_on_pi_bullseye/#install-basic-bot","title":"Install Basic Bot","text":"<p>Follow the instructions in Getting Started for installing and running basic_bot.</p>"},{"location":"Installation%20Guides/setup_on_pi_bullseye/#use-picamera2-instead-of-opencv-if-using-ribbon-cable-camera","title":"Use picamera2 instead of opencv if using ribbon cable camera","text":"<p>As of Feb 7, 2025, OpenCV camera capture will NOT work on Debian Bullseye or Bookworm with a ribbon cable camera.</p> <p>You must either use a USB camera or use the <code>basic_bot.commons.camera_picamera</code> module.</p> <p>See the API docs for using camera_picamera for more information about how to use.</p>"},{"location":"zzz/og_original_goals/","title":"Og original goals","text":"<p>The goals for this project are: 1. provide significantly simpler alternative to ROS 1. run stand-alone; decentralized 1. install and run on small SBCs like raspberry pi 1. distribute a pip package 1. maybe distribute a npm package for the typescript web components 1. simplify getting started on new projects.  This would ideally be something like: <pre><code>pip3 install basic_bot\npython3 basic_bot:create my_new_robot_project\n</code></pre> which would  -  - create a project directory (<code>mkdir my_new_robot_project</code>) with start, stop, upload scripts;  - cd to that directory  - create .gitignore file from basic python template with additions for logs/ pids/ etc.  - create logs/ directory  - create pids/ directory  - create src/ directory  - create webapp/ directory and populate it with a basic vite starter project in Typescript</p> <p>Much of the design originally will come from https://github.com/littlebee/strongarm and strip it down to just the parts that are used for all bots like central-hub, with optional submodules for things like Raspberry Pi system stats, servo control, motor control, vision, and image object recognition.</p> <p>My intent is to later update strongarm to use this package instead of it's own versions of some of these subsystems.</p>"},{"location":"zzz/validating_bookworm/","title":"Validating bookworm","text":"<p>I validated the setup_on_pi_bookworm.md by uploading daphbot-due to the Raspberry pi4 and pi5 (from my dev machine in the daphbot-due root project dir): <pre><code>./upload.sh pi5.local\n</code></pre></p>"},{"location":"zzz/validating_bookworm/#install-daphbot-due-dependencies","title":"Install daphbot-due dependencies","text":"<p>Install port audio (needed by sounddevice pip package) <pre><code>sudo apt-get install portaudio19-dev\npython -m pip install sounddevice\n</code></pre></p>"},{"location":"zzz/validating_bookworm/#install-requirements","title":"Install Requirements","text":"<p>ssh into pi5.local and: <pre><code>cd daphbot_due\npython -m pip install -r requirements.txt\nrm -Rf logs/*\n</code></pre></p>"},{"location":"zzz/validating_bookworm/#start-the-daphbot-services","title":"Start the daphbot services","text":"<pre><code>./start.sh\n</code></pre> <p>wait a sec and then clear the termimal, cat the logs and look for errors: <pre><code>clear\ncat logs/*\n</code></pre></p>"},{"location":"zzz/validating_bookworm/#it-all-worked-with-a-usb-camera","title":"It all worked!  with a USB camera","text":"<p>If the <code>basic_bot.services.vision</code> successfully started, you can open a browser and see the video capture and recognition stats at (http://pi5.local:5801/stats).</p> <p>I'm seeing some impressive stats!  Recognition is running at nearly 100% of the capture FPS at 24.79 frames per second.  Below are the stats: <pre><code>{\n    \"capture\": {\n        \"totalFramesRead\": 5938,\n        \"totalTime\": 240.3178722858429,\n        \"overallFps\": 24.70894046921789,\n        \"fpsStartedAt\": 1738212599.538883,\n        \"floatingFps\": 24.796676477203466\n    },\n    \"recognition\": {\n        \"last_objects_seen\": [\n            {\n                \"bounding_box\": [\n                    16,\n                    8,\n                    624,\n                    456\n                ],\n                \"classification\": \"person\",\n                \"confidence\": 0.515625\n            },\n        ],\n        \"fps\": {\n            \"totalFramesRead\": 5938,\n            \"totalTime\": 240.08938884735107,\n            \"overallFps\": 24.73245497648954,\n            \"fpsStartedAt\": 1738212599.7694898,\n            \"floatingFps\": 24.79684199179873\n        },\n        \"total_objects_detected\": 17774,\n        \"last_frame_duration\": 0.030208587646484375\n    }\n}\n</code></pre></p> <p>Note: the above was using a USB connected camera.</p>"}]}